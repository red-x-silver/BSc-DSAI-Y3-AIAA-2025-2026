<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
  <title>Artificial Intelligence and Advanced Analytics 10</title>
  <!-- This whole presentation was made with Big: https://github.com/tmcw/big -->
  
  <link href="../big/big.css" rel="stylesheet" type="text/css" />
  <script src="../big/big.js"></script>
  <link href="../big/themes/lightWhite.css" rel="stylesheet" type="text/css" />

  <!-- Favicon link below, via https://emojitofavicon.com -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%2210 0 100 100%22><text y=%22.90em%22 font-size=%2290%22>ğŸ´</text></svg>"></link>
  
  <style>
    body {
      background-color: #EDDD6E;
      font-family: -apple-system, BlinkMacSystemFont, avenir next, avenir, helvetica neue, helvetica, Ubuntu, roboto, noto, segoe ui, arial, sans-serif;
      /* font-weight: 700;*/
      margin: 0;
      padding: 0;
      font-style: normal;
      font-size: 21px;
    }
  </style>
  
</head>
<body>
  <div>
    AIAAğŸ‘
    <br />
    Lecture 10
    <br />
    Multi-modal AI
  </div>
    
  <div>Welcome ğŸ‘©â€ğŸ¤ğŸ§‘â€ğŸ¤ğŸ‘¨â€ğŸ¤</div>
  

  
  <div> First of all, don't forget to confirm your attendence on <a href="https://www.arts.ac.uk/study-at-ual/course-regulations/attendance-policy/attendance-monitoring"> SEAtS App!  </a></div>

<div>
  <h3>By the end of this lecture, we will have learned:</h3>
  <br />
  <br /> - What multimodal AI is and what it is for
  <br /> - Core multimodal representations (text, image, audio, video)
  <br /> - How cross-modal alignment works
  <br /> - Fusion architectures: early, mid, late fusion
  <br /> - How CLIP, BLIP-2, Gemini, and GPT-4o process multiple modalities
  <br />
</div>

    <div>
  <h3>What is Multi-Modal AI?</h3>
  <br />
  AI systems that learn from and generate multiple forms of data (modalities):
  <br />
  - Text
  - Images
  - Audio
  - Video
  - 3D
  - Sensor data (e.g., depth, heart rate)
  <br /><br />
  Goal: Build models that understand the world more like humans do, which means being able to take in and process information from different modalities.
</div>

  <div>
  <h3>Why Multi-Modal AI?</h3>
  <br />
  - Many real-world tasks involve multiple data sources (modalities): text, image, audio, etc. <br />
  - Richer context, which can lead to better understanding<br />
  - Improves robustness<br />
  - Enables natural human-AI interaction<br />
  - Foundation of modern generative AI (e.g., text-to-image, text-to-video)
</div>



  <div>
  <h3>Modalities and their representations (a summary of what we have talked about so far)</h3>
  <br />
  - Text: tokens, embeddings<br />
  - Image: pixels, patches, visual tokens<br />
  - Audio: waveforms, spectrograms<br />
  - Video: sequences of frames + audio<br />
  - 3D: point clouds, meshes, NeRFs<br />
  - Multisensory: physiological, depth, LiDAR
</div>

  <div>
  <h3>How to fuse the representations? Three approaches!</h3>
  <br />
  <b>Early Fusion</b>: merge raw features<br />
  <b>Mid Fusion</b>: merge learned embeddings<br />
  <b>Late Fusion</b>: merge final predictions<br /><br />
  Modern LMMs typically use mid-level fusion with cross-attention - let's zoom into this!
</div>

  
<div>
  <h3>Shared representation space</h3>
  <p>The core of multi-modal AI system is usually an embedding model, which can project different modalities into one aligned space:</p

  <br />
  text â†’ embedding<br />
  image â†’ embedding<br />
  audio â†’ embedding<br />
  video â†’ embedding<br />
</div>

  <div>
  <h3>Shared Embedding Space</h3>
  <pre>
      Text "a cat"   â†’ â€¢
      Image of cat   â†’ â€¢
      Audio "meow"   â†’ â€¢
      Video clip     â†’ â€¢

  "Shared": all collapse into nearby vectors that are close to each other in the embedding space, and this indicates aligned meanings.
  </pre>
</div>

     <div>
  <h3>Applications of cross-modal alignment</h3>
  <br />
  - Text â†” Image (captioning, retrieval)<br />
  - Text â†” Audio (speech & sound recognition)<br />
  - Text â†” Video (instruction following)<br /><br />

</div>

<div>
  <h3>The technique behind the shared embedding space/cross-modal alignment</h3>
  <br /><a href="https://en.wikipedia.org/wiki/Contrastive_Language-Image_Pre-training">Contrastive Learning</a>! (used in CLIP, <a href="https://github.com/facebookresearch/ImageBind">ImageBind</a>, etc.)
</div>
  
  
  <div>
  <h3><a href="https://en.wikipedia.org/wiki/Contrastive_Language-Image_Pre-training">Contrastive Learning (CL) </a></h3> 
  Contrastive Learning is the foundation of modern multi-modal AI.
  <ul>
    <li>A technique where the model learns by comparing pairs of data.</li>
    <li>Pulls matching pairs closer (positive pairs).</li>
    <li>Pushes mismatched pairs apart (negative pairs).</li>
    <li>Core idea behind multi-modal alignment (e.g., images â†” text).</li>
  </ul>

  <br /><b>Key maths behind it:</b> 
  <ul>
    <li>Similarity measured by dot-product or cosine similarity.</li>
    <li>Loss function include NCE, InfoNCE, and more on<a href="https://lilianweng.github.io/posts/2021-05-31-contrastive/">this nice tutorial</a> .</li>
  </ul>

  <br /><b>Used in models:</b>
  <ul>
    <li><a href="https://openai.com/index/clip/">CLIP</a> (imageâ€“text)</li>
    <li><a href="https://arxiv.org/abs/2106.13043">AudioCLIP</a> (audioâ€“imageâ€“text)</li>  
    <li><a href="https://www.youtube.com/watch?v=yuE4o2ut2Ck">VideoCLIP</a> (videoâ€“text)</li>
    <li><a href="https://github.com/facebookresearch/ImageBind">ImageBind</a> (unified multi-modal alignment)</li>
  </ul>
  
</div>


<div>
  <h3>Vision-Language Foundations</h3>
  <br />
  Example models:<br />
  - <a href="https://openai.com/index/clip/">CLIP</a><br />  
  - <a href="https://research.google/blog/align-scaling-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/">ALIGN</a><br />
  - <a href="https://github.com/salesforce/BLIP">BLIP</a> / <a href="https://wandb.ai/gladiator/BLIP-2/reports/BLIP-2-A-new-Visual-Language-Model-by-Salesforce--VmlldzozNjM0NjYz">BLIP-2</a><br />   
  - <a href="https://medium.com/@paluchasz/understanding-flamingo-visual-language-models-bea5eeb05268">Flamingo</a><br /><br />
  Pattern: pretrained vision encoder + pretrained LLM + adapter.
</div>

  <div>
  <h3><a href="https://openai.com/index/clip/">CLIP</a>: Contrastive Learning</h3>
  <br />
  - Train image encoder + text encoder together<br />
  - Contrastive objective aligns text and image embeddings<br />
  - Enables zero-shot image classification<br />
  - Foundation for modern text-to-image generation systems
</div>
  
  <div>
  <h3><a href="https://openai.com/index/clip/">CLIP</a> Intuition (Visual Diagram)</h3>
  <pre>
   Text:  "a dog running"      Image:  ğŸ•â€ğŸ¦ºğŸƒ
           â”‚                         â”‚
           â–¼                         â–¼
     Text Encoder              Image Encoder
           â”‚                         â”‚
           â””â”€â”€â”€â”€â–º Shared Embedding â—„â”€â”€â”€â”˜
                       Space
           â””â”€â”€â”€â”€ Contrastive Loss â”€â”€â”€â”€â”˜
  </pre>
  CLIP learns to bring matching textâ€“image pairs closer together.
</div>

  

  <div>
  <h3><a href="https://wandb.ai/gladiator/BLIP-2/reports/BLIP-2-A-new-Visual-Language-Model-by-Salesforce--VmlldzozNjM0NjYz">BLIP-2</a>: Bootstrapping Language-Image Pretraining</h3>
  <br />
  - Frozen Vision Transformer encoder<br />
  - Frozen LLM (e.g., OPT, FlanT5)<br />
  - Learned Q-Former maps image features â†’ tokens for LLM<br /><br />
  Enables instruction-following with multimodal inputs.
</div>

  <div>
  <h3>Modern Multimodal LLMs</h3>
  <br />
  - <a href="https://openai.com/index/hello-gpt-4o/">GPT-4o</a><br />  
  - <a href="https://gemini.google.com/">Google Gemini</a><br />  
  - <a href="https://llava-vl.github.io/">LLaVA</a><br />  
  - <a href="https://github.com/QwenLM/Qwen3-VL">Qwen-VL</a><br /><br />  
  Characteristics: unified encoder-decoder + cross-attention.
</div>

  

  <div>
  <h3>Multi-modal generation: Text-to-Image Models</h3>
  <br />
  - Diffusion Models (Stable Diffusion, Imagen, DALLÂ·E)<br />
  - Generative Transformers (OpenAI Sora pipeline uses hybrid components)<br /><br />
  Key technique: cross-attention between text embeddings and visual latents.
</div>

  <div>
  <h3>Multi-modal generation: Text-to-Audio Models</h3>
  <br />
  - Whisper: speech-to-text (check our lecture 04!)<br />  
  - <a href="https://huggingface.co/docs/diffusers/en/api/pipelines/audioldm">AudioLM</a> / <a href="https://github.com/suno-ai/bark">Bark</a>: text-to-speech<br />   
  - <a href="https://github.com/facebookresearch/ImageBind">ImageBind</a>: image â†” audio â†” video â†” IMU<br /><br />  
  Objective: unify audio with other modalities in shared space.
</div>

  <div>
  <h3>Multi-modal generation: Image-to-Text Model</h3>
  <br />
  - <a href="https://www.youtube.com/watch?v=yuE4o2ut2Ck">VideoCLIP</a><br />  
  - <a href="https://research.google/blog/locked-image-tuning-adding-language-understanding-to-image-models/">LiT</a><br />
  - <a href="https://sites.research.google/pali/">PaLI</a><br />
  - <a href="https://medium.com/@paluchasz/understanding-flamingo-visual-language-models-bea5eeb05268">Flamingo</a><br /><br />
</div>


  <div>
  <h3>Case study: <a href="https://openai.com/index/video-generation-models-as-world-simulators/">OpenAI Sora</a></h3>    
  <br />
  - Text â†’ Video generative model<br />
  - Video synthesis + physics-aware consistency<br />
  - Latent token transformer<br />
  - Multimodal conditioning: text, images, videos
</div>

  <div>
  <h3>Multimodal Tokenization</h3>
  <br />
  Emerging trend: convert every modality into discrete tokens<br />
  - VQ-VAE<br />
  - VQ-GAN<br />
  - Tokenizer for image, audio, video<br /><br />
  Enables unified transformer architecture.
</div>
  
    <div>
  <h3>Multi-modal with Transformer neural network </h3>
  <pre>
Tokens:
 [Text tokens] [Image tokens] [Audio tokens] ...

Transformer Layers:
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Self-attention across ALL modalities       â”‚
 â”‚  Cross-modal attention emerges naturally    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  </pre>
</div>

  <div>
  <h3>Multimodal Reasoning Capabilities</h3>
  <br />
  - Understanding charts, diagrams<br />
  - Visual question answering<br />
  - Image-based math problems<br />
  - Multi-step reasoning with visual grounding
</div>

 
  <div>
  <h3>Evaluation of Multi-Modal Models</h3>
  <br />
  - Zero-shot retrieval (image-text)<br />
  - VQA (visual question answering)<br />
  - Captioning metrics (CIDEr, BLEU)<br />
  - Video QA / temporal benchmarks<br />
  - Performance from real-world tasks: robotics, navigation
  <p>(essentially the evaluation of the subtasks that multi-modal model can do)</p>
</div>

  <div>
  <h3>Future of Multi-Modal AI (maybe)</h3>
  <br />
  - Unified <a href="https://www.nvidia.com/en-gb/glossary/world-models/">world models</a><br />  
  - Integration of 3D + physics + video<br />
  - Agents that perceive, act, and reason<br />
  - <a href="https://github.com/JiuTian-VL/Optimus-1">Long-horizon multimodal memory</a><br /><br />  
  Toward general-purpose "embodied" intelligence.
</div>


  
  
<div>
  <h3>âœ‹Hands-on steps for building a multi-modal seach app: </h3> 
  <br /> - system input: a collection of images, a piece of text as input query, 
  <br /> - Use the pre-trained CLIP embedding model to compute the embedding of every image from the collection (and store them in a database!) 
  <br /> - Use the pre-trained CLIP embedding model to compute the embedding of the input text
  <br /> - Compute the cosine similarity between the embeddings of every pair of an image and the input text
  <br /> - return the image that has the highest similarity score
</div>

  <div>
  <h3>âœ‹Recommended learning resources on multi-modal AI:</h3> 
  <p><a href="https://www.youtube.com/watch?v=WkoytlA3MoQ">this simple intro video (not a big fan of the "woman" example in it tho)</a>, <a href="https://encord.com/blog/open-ai-clip-alternatives/">this article </a> on CLIP and its alternatives and  <a href="https://www.youtube.com/playlist?list=PLc0Yh0D0XR4Z3wityRaEuu4rfzTHRIIAO">this MIT course on "How to AI (almost) anything" </a> (lec 4-8 on multi-modal)</p>
</div>
  
  
     
  <div> 
    Homework:
   <br />
    - Start preparing for your <a href="https://moodle.arts.ac.uk/mod/resource/view.php?id=1559297">final project</a>! 
  <br />
    - If you plan to work with content we have covered so far, send me a small proposal on what you plan to make and I'll recommend relevant resources!
  <br />
    - If you plan to work with content we have not covered so far, send me a message and I'll recommend early readings/preparation for you!
  </div>

  <div> 
  <h3>ğŸ•¶ï¸ What we have learnt today</h3>
  <br />
  <b>1. Multimodal AI Foundations</b><br />
  - What modalities are (text, image, audio, video, 3D)<br />
  - Why multimodality matters for real-world AI<br />
  - The idea of a shared embedding space<br />
  - Contrastive learning for cross-modal alignment<br /><br />

  <b>2. Key Models & Architectures</b><br />
  - CLIP for text-image embedding alignment<br />
  - BLIP-2 for image-to-language bridging<br />
  - Modern multimodal LLMs (GPT-4o, Gemini, LLaVA)<br />
  - Fusion types: early, mid, late fusion<br /><br />

  <b>3. Multimodal Generation</b><br />
  - Text-to-image diffusion models<br />
  - Text-to-audio models<br />
  - Image-to-text (Vision-language) models for reasoning and Q&A<br />

</div>

  
  <div> 
    We'll see you next Monday same time and same place!
  </div>
  
</body>
</html>
