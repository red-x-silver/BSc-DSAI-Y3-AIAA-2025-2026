<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
  <title>Artificial Intelligence and Advanced Analytics 06</title>
  <!-- This whole presentation was made with Big: https://github.com/tmcw/big -->
  
  <link href="../big/big.css" rel="stylesheet" type="text/css" />
  <script src="../big/big.js"></script>
  <link href="../big/themes/lightWhite.css" rel="stylesheet" type="text/css" />

  <!-- Favicon link below, via https://emojitofavicon.com -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%2210 0 100 100%22><text y=%22.90em%22 font-size=%2290%22>üç¥</text></svg>"></link>
  
  <style>
    body {
      background-color: #EDDD6E;
      font-family: -apple-system, BlinkMacSystemFont, avenir next, avenir, helvetica neue, helvetica, Ubuntu, roboto, noto, segoe ui, arial, sans-serif;
      /* font-weight: 700;*/
      margin: 0;
      padding: 0;
      font-style: normal;
      font-size: 21px;
    }
  </style>
  
</head>
<body>
  <div>
    AIAAüëê
    <br />
    Lecture 06
    <br />
    AI applications in 3D modelling ü•Å 
  </div>
    
  <div>Welcome üë©‚Äçüé§üßë‚Äçüé§üë®‚Äçüé§</div>
  
  <div> By the end of this lecture, we'll have learnt about:
  
     <br /> The theoretical: 
    <br /> - Introduction to 3D representations
     <br /> - Introduction to SOTA text-to-3D models
     <br /> - Introduction to SOTA image-to-3D models
    
     <br /> The practical: 
    <br /> - Hands-on: example on how to setup an AI project from github on our local machine, step-by-step
   
  
  </div>
  
  <div> First of all, don't forget to confirm your attendence on <a href="https://www.arts.ac.uk/study-at-ual/course-regulations/attendance-policy/attendance-monitoring"> SEAtS App!  </a></div>


<div>
  <h2>AI applications in 3D Modelling</h2>
  <p> Understanding how AI learns, represents, and generates 3D content.</p>
</div>

<div>
  <h3>Why 3D?</h3>
  <p>3D data underpins AR/VR, robotics, simulation, games, architecture, and design.</p>
  <p>AI enables automatic creation and understanding of 3D structures from limited data.</p>
</div>

<div>
  <h3>What is 3D Representation?</h3>
  <p>3D data describes geometry (shape) and appearance (texture/materials).</p>
  <p>There are multiple ways to represent 3D ‚Äî each with trade-offs in accuracy and efficiency.</p>
</div>

<div>
  <h3>Main Types of 3D Representations</h3>
  <ul>
    <li><b>Point Clouds:</b> sets of (x, y, z) points</li>
    <li><b>Meshes:</b> vertices + edges + faces</li>
    <li><b>Voxels:</b> 3D pixels in a grid</li>
    <li><b>Implicit Fields:</b> continuous functions (e.g., NeRF, SDF)</li>
  </ul>
</div>


<div>
  <h3><a href="https://en.wikipedia.org/wiki/Point_cloud">Point Clouds</a></h3>
  <p>Simple and flexible: just coordinates and sometimes color.</p>
  <p>- Used by LiDAR sensors and 3D scanners.</p>
  <p>- Challenge: irregular structure ‚Äî needs specialized neural networks (PointNet, PointNet++).</p>
</div>

<div>
  <h3><a href="https://en.wikipedia.org/wiki/Polygon_mesh">Meshes</a></h3>
  <p>Most common in graphics: connected vertices forming triangles.</p>
  <p>- Efficient and easy to render, but difficult for neural networks to learn directly.</p>
</div>


<div>
  <h3><a href="https://en.wikipedia.org/wiki/Voxel">Voxels</a></h3>
  <p>3D equivalent of 2D pixels ‚Äî uniform cubic grid.</p>
  <p>- Easy for CNNs but memory intensive (resolution grows cubically).</p>
</div>

<div>
  <h3>Implicit Representations</h3>
  <p>Represent 3D shapes as continuous functions f(x,y,z) ‚Üí value (e.g., density or signed distance).</p>
  <p>- Examples: <a href="https://www.matthewtancik.com/nerf">NeRF (Neural Radiance Fields)</a>.</p>
  <p>- Compact and differentiable ‚Äî great for generative AI.</p>
</div>


<div>
  <h3>Applications of AI in 3D</h3>
  <ul>
    <li>- Reconstruction (from images or scans)</li>
    <li>- Generation (new 3D assets from text or 2D input)</li>
    <li>- Editing and style transfer</li>
    <li>- Simulation and robotics perception</li>
  </ul>
</div>

<div>
  <h2>Image-to-3D Generation</h2>
  <p>Goal: infer a full 3D model from one or more 2D images.</p>
  <p>Challenge: 3D structure is not directly visible ‚Äî need prior knowledge and multi-view reasoning.</p>
</div>


  <div>
  <h3>Common Techniques in Image-to-3D</h3>
  <ul>
    <li> - Neural Radiance Fields (NeRF) and variants.</li>
    <li>3D Gaussian Splatting for real-time rendering</li>
    <li>Diffusion models trained on 2D‚Äì3D paired data</li>
    <li>Multi-view supervision using differentiable rendering loss</li>
  </ul>
</div>

<div>
  <h3>Example: <a href="https://www.matthewtancik.com/nerf">NeRF</a> (2020)</h3>
  <p>Neural Radiance Field = MLP mapping (x,y,z,view_dir) ‚Üí color + density.</p>
  <p>- Optimized to reproduce images from multiple camera views.</p>
  <p>- Revolutionized view synthesis and 3D reconstruction.</p>
  <p> -  <a href="https://docs.nerf.studio/index.html">NeRF Studio</a>: a simple API that allows for a simplified end-to-end process of creating, training, and testing NeRFs.</p>
  
</div>


<div>
  <h3>Example:  <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">Gaussian Splatting</a></h3>
  <p>(a gentle introduction <a href="https://huggingface.co/blog/gaussian-splatting">here</a> and <a href="https://www.youtube.com/watch?v=HVv_IQKlafQ">here</a>)</p>
  <p>A modern 3D representation using <b>3D Gaussian primitives</b> instead of voxels or meshes.</p>
  <ul>
    <li>Each Gaussian stores position, color, opacity, and scale (like a soft 3D blob).</li>
    <li>Scenes are represented by millions of tiny overlapping Gaussians.</li>
    <li>Rendering = projecting and blending these Gaussians from a camera view.</li>
  </ul>
  <p><b>Advantages:</b> Real-time rendering (60+ FPS), continuous surfaces, photorealistic results.</p>
  <p><b>Used in:</b> 3D reconstruction (NeRF‚ÜíGaussian), image-to-3D diffusion models (LGM, GaussianDreamer).</p>
</div>


  

<div>
  <h3>Image-to-3D SOTA Examples (2023‚Äì2025)</h3>
  
  <ul>
    <li><b><a href="https://github.com/cvlab-columbia/zero123">Zero123++</a> / <a href="https://github.com/SUDO-AI-3D/zero123plus">Zero123++</a></b> ‚Äì diffusion model generating novel views from one image</li>
    <li><b><a href="https://github.com/xxlong0/Wonder3D">Wonder3D</a></b> ‚Äì multi-view diffusion with depth conditioning</li> 
    <li><b><a href="https://github.com/3DTopia/LGM">LGM (Large Gaussian Model)</a></b> ‚Äì Gaussian-based 3D representation for fast generation</li>   
  </ul>
</div>



<div>
  <h2>Text-to-3D Generation</h2>
  <p>Goal: create a 3D object directly from a text prompt like ‚Äúa red chair with golden legs‚Äù.</p>
  <p> - Combines text understanding (via CLIP or language models) and 3D synthesis (via NeRF or diffusion).</p>
</div>

  <div>
  <h3>Challenges of text-to-3D</h3>
  <ul>
    <li>Training data scarcity for 3D</li>
    <li>High computation and memory cost</li>
    <li>Ambiguity of text (multiple valid 3D outputs given the same prompt)</li>
  </ul>
</div>
  
<div>
  <h3>Techniques Behind Text-to-3D</h3>
  <ul>
    <li>CLIP or LLM for semantic understanding</li>
    <li>Score Distillation Sampling (SDS) from 2D diffusion models</li>
    <li>3D Gaussian Splatting for differentiable scene representation</li>
    <li>Latent NeRF or mesh optimization guided by text-image loss</li>
  </ul>
</div>

<div>
  <h3>Early Text-to-3D models</h3>
  <ul>
    <li><b><a href="https://ajayj.com/dreamfields">Dream Fields (2021)</a>:</b> optimize NeRF guided by CLIP text-image similarity.</li>
  </ul>
</div>

<div>
  <h3>Recent approach: Diffusion-Based Text-to-3D Models</h3>
  <p>Recent models use 2D diffusion models as priors to guide 3D generation.</p>
  <ul>
    <li>Score distillation sampling (SDS)</li>
    <li>Latent diffusion (as in Stable Diffusion)</li>
    <li>Gaussian splatting for fast, consistent geometry</li>
  </ul>
</div>


<div>
  <h3>State-of-the-Art Text-to-3D (2024‚Äì2025)</h3>
  <ul>
    <li><b><a href="https://dreamfusion3d.github.io/">DreamFusion</a></b>: text-to-image diffusion + NeRF</li>
    <li><b><a href="https://research.nvidia.com/labs/dir/magic3d/">Magic3D (NVIDIA)</a></b>: two-stage coarse-to-fine optimization</li>
    <li><b><a href="https://github.com/Gorilla-Lab-SCUT/Fantasia3D">Fantasia3D</a></b>: explicit geometry and texture optimization</li>
    <li><b><a href="https://github.com/hustvl/GaussianDreamerPro">GaussianDreamer Pro</a></b> real-time diffusion-based text-to-3D, also for animation</li>
  </ul>
</div>

 <div>
  <h3>Hunyuan3D (Tencent, 2024)</h3>
  <p>A state-of-the-art <b>text-to-3D & image-to-3D diffusion model</b> built on the Tencent Hunyuan foundation model.</p>
  <p> Hugging face spaces available for <a href="https://huggingface.co/spaces/tencent/Hunyuan3D-2.1">Hunyuan3D 2.1 (img-to-3D) </a> and <a href="https://huggingface.co/spaces/tencent/Hunyuan3D-2">Hunyuan3D 2.0 (img/text to 3D) </a> </p>
</div>
  
  <div>
  <h3>Hunyuan3D (Tencent, 2024)</h3>
    <ul>
    <li>- Uses a powerful <b>multi-stage diffusion process</b> to generate consistent 3D geometry and texture.</li>
    <li>- Employs <b>Gaussian Splatting</b> as its 3D representation for fast optimization and rendering.</li>
    <li>- Trained with <b>multi-view diffusion priors</b> and <b>3D geometry regularization</b> to ensure shape fidelity.</li>
    <li>- Can generate <b>high-quality 3D assets</b> directly from text or single images with realistic materials.</li>
  </ul>
 
</div>



<div>
  <h3>Emerging Trends</h3>
  <ul>
    <li>3D diffusion models trained natively on 3D datasets</li>
    <li>Multi-modal foundation models combining text, image, video, and 3D</li>
    <li>Integration with generative design and digital twins</li>
  </ul>
</div>

<div>
  <h3>Hands-on</h3> 
  <p>how to have an AI model from github to run locally on our machines? - using <a href="https://github.com/openai/shap-e?tab=readme-ov-file">Shap_e from OpenAI on github </a> as an example</p>
  <ul>
    <li>Read the Readmd file for instructions!</li>
    <li>Download the code</li>
    <li>Prepare the environment (I'll demo)</li>
    <li>Run the code provided for inference (I'll demo)</li>
  </ul>
</div>

     
  <div> 
    Homework:
   <br />
    - Start preparing for your <a href="https://moodle.arts.ac.uk/mod/resource/view.php?id=1559297">final project</a>! 
  <br />
    - If you plan to work with content we have covered so far, send me a small proposal on what you plan to make and I'll recommend relevant resources!
  <br />
    - If you plan to work with content we have not covered so far, send me a message and I'll recommend early readings/preparation for you!
  </div>

   <div> 
    üï∂Ô∏è What we have learnt today:
        <br />
   - 3D representations: point clouds, meshes, voxels, implicit fields
     <br />
    - Image-to-3D: NeRFs, multi-view diffusion, Gaussian splats
<br />
     - Text-to-3D: CLIP-guided diffusion, DreamFusion, Magic3D, Fantasia3D
      <br />
   - Core techniques: SDS, differentiable rendering, latent diffusion
<br />
     - Hands-on: a pipeline for working with models from github -> checkout the code repo, prepare environment, run the code provided for model inference 
  </div>
  
  <div> 
    We'll see you next Monday same time and same place!
  </div>
  
</body>
</html>
