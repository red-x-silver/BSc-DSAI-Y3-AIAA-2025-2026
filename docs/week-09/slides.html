<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
  <title>Artificial Intelligence and Advanced Analytics 09</title>
  <!-- This whole presentation was made with Big: https://github.com/tmcw/big -->
  
  <link href="../big/big.css" rel="stylesheet" type="text/css" />
  <script src="../big/big.js"></script>
  <link href="../big/themes/lightWhite.css" rel="stylesheet" type="text/css" />

  <!-- Favicon link below, via https://emojitofavicon.com -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%2210 0 100 100%22><text y=%22.90em%22 font-size=%2290%22>üç¥</text></svg>"></link>
  
  <style>
    body {
      background-color: #EDDD6E;
      font-family: -apple-system, BlinkMacSystemFont, avenir next, avenir, helvetica neue, helvetica, Ubuntu, roboto, noto, segoe ui, arial, sans-serif;
      /* font-weight: 700;*/
      margin: 0;
      padding: 0;
      font-style: normal;
      font-size: 21px;
    }
  </style>
  
</head>
<body>
  <div>
    AIAAüëê
    <br />
    Lecture 09
    <br />
    AI applications in affective computing + introduction to CCI workstations
  </div>
    
  <div>Welcome üë©‚Äçüé§üßë‚Äçüé§üë®‚Äçüé§</div>
  

  
  <div> First of all, don't forget to confirm your attendence on <a href="https://www.arts.ac.uk/study-at-ual/course-regulations/attendance-policy/attendance-monitoring"> SEAtS App!  </a></div>

<div>
  <h3>AI in Affective Computing</h3>
  <p>Understanding and modelling human emotions for human‚Äìcomputer interaction.</p>
  <ul>
    <li>What is affective computing?</li>
    <li>Emotion representation models</li>
    <li>Affective computing using data from vision, audio, text or physiology</li>
    <li>Multimodal emotion recognition</li>
    <li>Emotion-aware generation</li>
    <li>Ethical considerations</li>
  </ul>
</div>
  
<div>
  <h3>What is Affective Computing?</h3>
  <ul>
    <li>Field that studies how machines can detect, interpret, and simulate human emotions. üíú</li>
    <li>Key tasks:
      <ul>
        <li>Emotion recognition (via classification/regression)</li>
        <li>Emotion generation</li>
        <li>Affective interaction ‚Äî adapting responses based on emotional cues</li>
      </ul>
    </li>
    <li>Applications: recommender systems, games, chatbots, education, mental health, robotics, etc. </li>
  </ul>
</div>

<div>
  <h3>Why Emotions Matter üòé</h3>
  <ul>
    <li>Human communication is <b>not</b> only verbal ‚Äî body language + tone + words.</li>
    <li>AIs that fail to detect and/or demonstrate emotion often feel robotic or insensitive.</li>
    <li>Affective AI improves:
      <ul>
        <li>Empathy in agents</li>
        <li>User engagement</li>
        <li>Personalisation</li>
        <li>Safety in sensitive domains</li>
      </ul>
    </li>
  </ul>
</div>

<div>
  <h3>Emotion Representation Models (from Psychology!)</h3>
  <p>How do we define and measure ‚Äúemotion‚Äù for AI models?</p>
  <ul>
    <li><a href="https://en.wikipedia.org/wiki/Discrete_emotion_theory">Discrete Categories</a> : anger, fear, sadness, happiness, disgust, surprise (Ekman)</li>
    <li><a href="https://www.sciencedirect.com/topics/psychology/circumplex-model">Circumplex Model</a>: 
      <ul>
        <li>Valence (positive ‚Üí negative)</li>
        <li>Arousal (calm ‚Üí excited)</li>
        <li>[optional] Dominance (submissive ‚Üí dominant)</li>
      </ul>
    </li> 
    <li><a href="https://en.wikipedia.org/wiki/Appraisal_theory">Appraisal Theory</a>: emotions arise from evaluations of events that cause specific reactions in different people </li>
  </ul>
</div>
  
<div>
  <h3>Modalities in Affective Computing</h3>
  <ul>
    <li><b>Facial expressions</b> ‚Äî computer vision</li>
    <li><b>Speech & voice</b> ‚Äî prosody, tone, pitch, rhythm</li>
    <li><b>Text</b> ‚Äî sentiment, emotion words, context</li>
    <li><b>Physiological signals</b> ‚Äî heart rate, <a href="https://en.wikipedia.org/wiki/Electrodermal_activity">EDA</a>, <a href="https://en.wikipedia.org/wiki/Electroencephalography">EEG</a></li>
    <li><b>Multimodal approaches</b> ‚Äî combining modalities for better accuracy</li>
  </ul>
</div>

  <div>
  <h3>Facial Emotion Recognition</h3>
  <ul>
    <li>Input: video frames or facial landmarks</li>
    <li>Examples: <a href="https://huggingface.co/ElenaRyumina/face_emotion_recognition">this</a> on hugging face, <a href="https://arxiv.org/abs/2403.13731">this</a> research work and <a href="https://github.com/mujiyantosvc/Facial-Expression-Recognition-FER-for-Mental-Health-Detection-">this</a> Github repo. </li>    
    <li>Common model options: CNNs,Vision Transformer </li>
    <li>Limitations: cultural differences, masking, lighting, occlusion</li>
  </ul>
</div>

  <div>
  <h3>Speech Emotion Recognition (SER)</h3>
  <ul>
    <li>Input audio features:
      <ul>
        <li>Prosodic: pitch, energy, rhythm</li>
        <li>Spectral: MFCCs, mel-spectrograms</li>
        <li>Voice quality: jitter, breathiness</li>
      </ul>
    </li>
    <li>Examples: <a href="https://www.kaggle.com/code/shivamburnwal/speech-emotion-recognition">this</a> Kaggle notebook, <a href="https://www.youtube.com/watch?v=s5-8yeYJV7Y">this</a> YTB tutorial and <a href="https://github.com/x4nth055/emotion-recognition-using-speech?tab=readme-ov-file">this</a> Github repo. </li>    
    <li>Common model options:CNNs, RNNs, Audio transformers (e.g., wav2vec2, Whisper)</li>
    <li>Limitations: cultural differences, ambiguity</li>
  </ul>
</div>

<div>
  <h3>Text Emotion Recognition</h3>
  <ul>
    <li>Sentiment analysis (early precursor!)</li>
    <li>Examples: <a href="https://www.kaggle.com/code/shivamburnwal/speech-emotion-recognition">this</a> Kaggle notebook, <a href="https://www.youtube.com/watch?v=s5-8yeYJV7Y">this</a> YTB tutorial and <a href="https://github.com/x4nth055/emotion-recognition-using-speech?tab=readme-ov-file">this</a> Github repo. </li>    
    <li>Common model options:
      <ul>
        <li>BERT, RoBERTa, DeBERTa</li>
        <li>LLMs with emotion classification heads (extra layers at the end of the neural network)</li>
      </ul>
    </li>
    <li>Challenges:
      <ul>
        <li>Irony, sarcasm</li>
        <li>No prosody or facial cues</li>
      </ul>
    </li>
  </ul>
</div>

  <div>
  <h3>Physiological Emotion Recognition</h3>
    
  <ul>
    <li>Signals:
      <ul>
        <li>ECG & heart rate variability</li>
        <li>Electrodermal activity (skin conductance)</li>
        <li>EEG/brain signals</li>
      </ul>
    </li>
    <li>Used in:
      <ul>
        <li>Stress detection</li>
        <li>Wellbeing apps</li>
        <li>Assistive technology</li>
      </ul>
    </li>
        <li>Examples: <a href="https://www.kaggle.com/code/shivamburnwal/speech-emotion-recognition">this</a> Kaggle notebook, <a href="https://www.youtube.com/watch?v=s5-8yeYJV7Y">this</a> YTB tutorial and <a href="https://github.com/x4nth055/emotion-recognition-using-speech?tab=readme-ov-file">this</a> Github repo. </li>    

  </ul>
</div>

  <div>
  <h3>Multimodal Emotion Recognition</h3>
  <ul>
    <li>Combines face + voice + text + physiological signals</li>
    <li>Techniques:
      <ul>
        <li>Feature-level fusion</li>
        <li>Late fusion (ensemble of modalities)</li>
        <li>Transformer-based cross-modal attention</li>
      </ul>
    </li>
    <li>State-of-the-art: multimodal transformers</li>
  </ul>
</div>

  <div>
  <h3>Datasets in Affective Computing</h3>
  <ul>
    <li><b>Facial:</b> FER+, AffectNet</li>
    <li><b>Speech:</b> RAVDESS, IEMOCAP</li>
    <li><b>Text:</b> GoEmotions</li>
    <li><b>Multimodal:</b> MELD, CMU-MOSEI, CMU-MOSI</li>
  </ul>
  <p>Important: many datasets are acted, not natural ‚Üí limited realism.</p>
</div>

  <div>
  <h3>Emotion Prediction in Games</h3>
  <ul>
    <li>Detect player frustration or engagement</li>
    <li>Adjust difficulty dynamically</li>
    <li>NPCs reacting emotionally to player actions</li>
    <li>Emotion-driven quest systems</li>
  </ul>
</div>
<div>
  <h3>Emotion-Aware Voice Generation</h3>
  <ul>
    <li>Models: StyleTTS, VITS, GPT-SoVITS</li>
    <li>Controls:
      <ul>
        <li>Valence, arousal</li>
        <li>Speaking style: sad, angry, happy, whispering</li>
      </ul>
    </li>
    <li>Applications: empathetic chatbots, game NPC voice acting</li>
  </ul>
</div>
<div>
  <h3>Emotion-Aware Chatbots</h3>
  <ul>
    <li>Detect emotion from user input (text + voice)</li>
    <li>Select empathetic responses</li>
    <li>Modulate tone and expressiveness in generation</li>
    <li>Applications: customer support, health, education</li>
  </ul>
</div>
<div>
  <h3>Large Models for Emotion Understanding</h3>
  <ul>
    <li>LLMs with emotional reasoning</li>
    <li>Vision-Language models (VLMs) detecting affect in images</li>
    <li>Emotion embeddings (e.g., valence/arousal vectors)</li>
    <li>Emergent ability: empathy simulation</li>
  </ul>
</div>
<div>
  <h3>Bias and Cultural Differences</h3>
  <ul>
    <li>Emotional expression varies by culture, gender, age</li>
    <li>Datasets often Western-centric</li>
    <li>Risk of misclassification ‚Üí unfair decisions</li>
    <li>Mitigation: diverse data, human oversight</li>
  </ul>
</div>
  
<div>
  <h3>Ethical Challenges</h3>
  <ul>
    <li>Emotion detection can be sensitive and intrusive</li>
    <li>Consent and transparency required</li>
    <li>Potential misuse:
      <ul>
        <li>Manipulation in advertising</li>
        <li>Surveillance</li>
        <li>Unwanted emotional profiling</li>
      </ul>
    </li>
  </ul>
</div>
  
<div>
  <h3>Limitations of Affective Computing</h3>
  <ul>
    <li>Emotions are ambiguous and complex</li>
    <li>Models often infer ‚Äúexpressions,‚Äù not true internal states</li>
    <li>Context heavily influences interpretation</li>
    <li>Cannot replace human judgment or empathy</li>
  </ul>
</div>

  

  

<div>
  <h3>‚úãHands-on</h3> 
  <p>A slightly tricky thing: for fun AI application in game, a fun game implementation is needed... </p>
 <p>For research purpose, we'll start with the barebone python implementation(no nice GUI yet sadly)</p>
  <p>let's go check some examples of AI agent RL training from <a href="https://github.com/datamllab/awesome-game-ai?tab=readme-ov-file">this platform</a></p>
  <ul>
    <li>Read the Readmd file for instructions!</li>
    <li>Pick a simple game to start with</li>
    <li>Run the cells on the google colab</li>
  </ul>
</div>

     
  <div> 
    Homework:
   <br />
    - Start preparing for your <a href="https://moodle.arts.ac.uk/mod/resource/view.php?id=1559297">final project</a>! 
  <br />
    - If you plan to work with content we have covered so far, send me a small proposal on what you plan to make and I'll recommend relevant resources!
  <br />
    - If you plan to work with content we have not covered so far, send me a message and I'll recommend early readings/preparation for you!
  </div>

   <div> 
    üï∂Ô∏è What we have learnt today:
       
      <br /> - Affective computing models human emotion in machines
       <br /> - Modalities: face, speech, text, physiology, multimodal
      <br /> - Used in games, wellbeing, education, companionship, etc.
     <br /> - High potential but high ethical responsibility
  </div>


  
  
  <div> 
    We'll see you next Monday same time and same place!
  </div>
  
</body>
</html>
