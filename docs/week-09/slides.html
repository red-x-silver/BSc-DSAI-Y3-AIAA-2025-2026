<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
  <title>Artificial Intelligence and Advanced Analytics 09</title>
  <!-- This whole presentation was made with Big: https://github.com/tmcw/big -->
  
  <link href="../big/big.css" rel="stylesheet" type="text/css" />
  <script src="../big/big.js"></script>
  <link href="../big/themes/lightWhite.css" rel="stylesheet" type="text/css" />

  <!-- Favicon link below, via https://emojitofavicon.com -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%2210 0 100 100%22><text y=%22.90em%22 font-size=%2290%22>üç¥</text></svg>"></link>
  
  <style>
    body {
      background-color: #EDDD6E;
      font-family: -apple-system, BlinkMacSystemFont, avenir next, avenir, helvetica neue, helvetica, Ubuntu, roboto, noto, segoe ui, arial, sans-serif;
      /* font-weight: 700;*/
      margin: 0;
      padding: 0;
      font-style: normal;
      font-size: 21px;
    }
  </style>
  
</head>
<body>
  <div>
    AIAAüëê
    <br />
    Lecture 09
    <br />
    AI applications in affective computing + introduction to CCI workstations
  </div>
    
  <div>Welcome üë©‚Äçüé§üßë‚Äçüé§üë®‚Äçüé§</div>
  

  
  <div> First of all, don't forget to confirm your attendence on <a href="https://www.arts.ac.uk/study-at-ual/course-regulations/attendance-policy/attendance-monitoring"> SEAtS App!  </a></div>

<div>
  <h3>AI in Affective Computing</h3>
  <p>Understanding and modelling human emotions for human‚Äìcomputer interaction.</p>
  <ul>
    <li>What is affective computing?</li>
    <li>Emotion representation models</li>
    <li>Affective computing using data from vision, audio, text or physiology</li>
    <li>Multimodal emotion recognition</li>
    <li>Emotion-aware generation</li>
    <li>Ethical considerations</li>
  </ul>
</div>
  
<div>
  <h3>What is Affective Computing?</h3>
  <ul>
    <li>Field that studies how machines can detect, interpret, and simulate human emotions. üíú</li>
    <li>Key tasks:
      <ul>
        <li>Emotion recognition (via classification/regression)</li>
        <li>Emotion simulation/generation</li>
        <li>Affective interaction ‚Äî adapting responses based on emotional cues</li>
      </ul>
    </li>
    <li>Applications: recommender systems, games, chatbots, education, mental health, robotics, etc. </li>
  </ul>
</div>

<div>
  <h3>Why Emotions Matter üòé</h3>
  <ul>
    <li>Human communication is <b>not</b> only verbal ‚Äî body language + tone + words.</li>
    <li>AIs that fail to detect and/or demonstrate emotion often feel robotic or insensitive.</li>
    <li>Affective AI improves:
      <ul>
        <li>Empathy in agents</li>
        <li>User engagement</li>
        <li>Personalisation</li>
        <li>Safety in sensitive domains</li>
      </ul>
    </li>
  </ul>
</div>

<div>
  <h3>Emotion Representation Models (from Psychology!)</h3>
  <p>How do we define and measure ‚Äúemotion‚Äù?</p>
  <ul>
    <li><a href="https://en.wikipedia.org/wiki/Discrete_emotion_theory">Discrete model</a> : anger, fear, sadness, happiness, disgust, surprise (Ekman)</li>
    <li><a href="https://www.sciencedirect.com/topics/psychology/circumplex-model">Circumplex Model</a>: 
      <ul>
        <li>Valence (positive ‚Üí negative)</li>
        <li>Arousal (calm ‚Üí excited)</li>
        <li>[optional] Dominance (submissive ‚Üí dominant)</li>
      </ul>
    </li> 
    <li><a href="https://en.wikipedia.org/wiki/Appraisal_theory">Appraisal Theory</a>: emotions arise from evaluations of events that cause specific reactions in different people </li>
  </ul>
</div>
  
<div>
  <h3>Data Modalities in Affective Computing üíø</h3>
  <ul>
    <li><b>Facial expressions</b> ‚Äî computer vision</li>
    <li><b>Speech & voice</b> ‚Äî prosody, tone, pitch, rhythm</li>
    <li><b>Text</b> ‚Äî sentiment, emotion words, context</li>
    <li><b>Physiological signals</b> ‚Äî heart rate, <a href="https://en.wikipedia.org/wiki/Electrodermal_activity">EDA</a>, <a href="https://en.wikipedia.org/wiki/Electroencephalography">EEG</a></li>
    <li><b>Multimodal approaches</b> ‚Äî combining modalities for better accuracy</li>
  </ul>
</div>

  <div>
  <h3>Facial Emotion Recognition üôÑ</h3>
  <ul>
    <li>Input: video frames or facial landmarks</li>
    <li>Examples: <a href="https://huggingface.co/ElenaRyumina/face_emotion_recognition">this</a> on hugging face, <a href="https://arxiv.org/abs/2403.13731">this</a> research work and <a href="https://github.com/mujiyantosvc/Facial-Expression-Recognition-FER-for-Mental-Health-Detection-">this</a> Github repo. </li>    
    <li>Common model options: CNNs,Vision Transformer </li>
    <li>Limitations: cultural differences, masking, lighting, occlusion</li>
  </ul>
</div>

  <div>
  <h3>Speech Emotion Recognition (SER) üóØÔ∏è</h3>
  <ul>
    <li>Input audio features:
      <ul>
        <li>Prosodic: pitch, energy, rhythm</li>
        <li>Spectral: MFCCs, mel-spectrograms</li>
        <li>Voice quality: jitter, breathiness</li>
      </ul>
    </li>
    <li>Examples: <a href="https://www.kaggle.com/code/shivamburnwal/speech-emotion-recognition">this</a> Kaggle notebook, <a href="https://www.youtube.com/watch?v=s5-8yeYJV7Y">this</a> YTB tutorial and <a href="https://github.com/x4nth055/emotion-recognition-using-speech?tab=readme-ov-file">this</a> Github repo. </li>    
    <li>Common model options:CNNs, RNNs, Audio transformers (e.g., wav2vec2, Whisper)</li>
    <li>Limitations: cultural differences, ambiguity</li>
  </ul>
</div>

<div>
  <h3>Text Emotion Recognition üí¨</h3>
  <ul>
    <li>Sentiment analysis (early precursor!)</li>
    <li>Examples: <a href="https://huggingface.co/tabularisai/multilingual-sentiment-analysis">this</a> model on Hugging Face, <a href="https://huggingface.co/siebert/sentiment-roberta-large-english">this</a> model on Hugging Face tutorial and <a href="https://tessa-deployed.streamlit.app/">this</a> App from Github (prediction not so good though) </li>    
    <li>Common model options:
      <ul>
        <li>pre-trained language models: BERT, RoBERTa, DeBERTa</li>
        <li>LLMs with emotion classification heads (extra layers at the end of the neural network)</li>
      </ul>
    </li>
    <li>Challenges:
      <ul>
        <li>Irony, sarcasm</li>
        <li>No prosody or facial cues</li>
      </ul>
    </li>
  </ul>
</div>

  <div>
  <h3>Physiological Emotion Recognitionü´Ä</h3>
    
  <ul>
    <li>Signals:
      <ul>
        <li>ECG & heart rate variability</li>
        <li>Electrodermal activity: <a href="https://en.wikipedia.org/wiki/Electrodermal_activity">EDA</a> (skin conductance)</li>
        <li><a href="https://en.wikipedia.org/wiki/Electroencephalography">EEG</a> (brain signals) </li>
      </ul>
    </li>
  <li> Devices for collecting physio data:smartwatches/bands, wearable sensors, etc. </li>
    <li>Used in:
      <ul>
        <li>Stress detection</li>
        <li>Wellbeing apps</li>
        <li>Assistive technology</li>
      </ul>
    </li>
    <li>Examples: <a href="https://www.nature.com/articles/s41597-024-03429-3">this</a> article from Nature, <a href="https://arxiv.org/abs/2205.10466">this</a> survey paper and <a href="https://www.youtube.com/watch?v=NeZE34GiMII">this</a> work of detecting fear from gaming. </li>    
  </ul>
</div>

  <div>
  <h3>Multimodal Emotion Recognition ‚öîÔ∏è </h3>
  <ul>
    <li>Combines face + voice + text + physiological signals</li>
    <li>How to combine:
      <ul>
        <li>Feature-level fusion (in early layers)</li>
        <li>Late fusion (ensemble of modalities in later layers)</li>
        <li>Transformer-based cross-modal attention</li>
      </ul>
    </li>
    <li>State-of-the-art: multimodal transformers</li>
    <li>Related work: <a href="https://arxiv.org/abs/2409.07388">this</a>survey paper,<a href="https://www.youtube.com/watch?v=Tq7mCiaFwB8">this</a> lecture on YTB </li>
  </ul>
</div>

  <div>
  <h3>Datasets in Affective Computing</h3>
  <ul>
    <li><b>Facial:</b>  <a href="https://github.com/microsoft/FERPlus">FER+</a>, <a href="https://www.mohammadmahoor.com/pages/databases/affectnet/">AffectNet</a></li>
    <li><b>Speech:</b> <a href="https://zenodo.org/record/1188976">RAVDESS</a>, <a href="https://tspace.library.utoronto.ca/handle/1807/24487">TESS</a></li>
    <li><b>Text:</b> <a href="https://www.kaggle.com/datasets/debarshichanda/goemotions">GoEmotions</a></li>
    <li><b>Multimodal:</b> <a href="https://sail.usc.edu/iemocap/">IEMOCAP</a>, <a href="https://affective-meld.github.io/">MELD</a>, <a href="https://www.kaggle.com/datasets/samarwarsi/cmu-mosei">CMU-MOSEI</a>, <a href="https://www.kaggle.com/datasets/mathurinache/cmu-mosi">CMU-MOSI</a></li>
  </ul>
  <p>Important: many datasets are "acted", not natural ‚Üí limited realism.</p>
</div>

  <div>
  <h3>Emotion Prediction in Games</h3>
  <ul>
    <li>Detect player frustration or engagement</li>
    <li>Adjust difficulty dynamically</li>
    <li>NPCs reacting emotionally to player actions</li>
    <li>Emotion-driven quest systems</li>
  </ul>
<p>Related works: <a href="https://arxiv.org/pdf/2312.06925">this paper</a>, <a href="https://www.youtube.com/watch?v=_lp0libfp5M">Press Y to Cry: Generating Emotions in Videogame Narrative</a></p>
</div>
  
<div>
  <h3>Emotion-Aware Voice Generation</h3>
  <ul>
    <li>Related works: <a href="https://huggingface.co/spaces/ResembleAI/Chatterbox">Chatterbox</a>, <a href="https://github.com/RVC-Boss/GPT-SoVITS?tab=readme-ov-file">GPT-SoVITS</a></li>
    <li>Controls:
      <ul>
        <li>Valence, arousal</li>
        <li>Speaking style: sad, angry, happy, whispering</li>
      </ul>
    </li>
    <li>Applications: empathetic chatbots, game NPC voice acting</li>
  </ul>
</div>
<div>
  <h3>Emotion-Aware Chatbots</h3>
  <ul>
    <li>Detect emotion from user input (text + voice)</li>
    <li>Opt for empathetic responses</li>
    <li>Modulate tone and expressiveness in generation</li>
    <li>Applications: customer support, caring, education</li>
  </ul>
</div>

  
<div>
  <h3>Bias and Cultural Differences</h3>
  <ul>
    <li>Emotional expression varies by culture, gender, age</li>
    <li>Datasets often Western-centric</li>
    <li>Risk of misclassification ‚Üí unfair decisions</li>
    <li>Mitigation: diverse data, human supervision</li>
  </ul>
</div>
  
<div>
  <h3>Ethical Challenges</h3>
  <ul>
    <li>Emotion detection can be sensitive and intrusive</li>
    <li>Consent and transparency required</li>
    <li>Potential misuse:
      <ul>
        <li>Manipulation in advertising</li>
        <li>Surveillance</li>
        <li>Unwanted emotional profiling</li>
      </ul>
    </li>
  </ul>
</div>
  
<div>
  <h3>Limitations of Affective Computing</h3>
  <ul>
    <li>Emotions are ambiguous and complex</li>
    <li>Models often infer ‚Äúexpressions,‚Äù not true internal states</li>
    <li>Context heavily influences interpretation</li>
    <li>Cannot replace human judgment or empathy</li>
  </ul>
</div>

  

  

<div>
  <h3>‚úãHands-on</h3> 
  <p>Let's familiarise ourselves with our beefy CCI workstations!</p>
  <p>Use whatever is free! There is also a booking system - our Orb system (underused). </p>
  <p>Instruction <a href="https://moodle.arts.ac.uk/mod/resource/view.php?id=1569012">here</a></p>
</div>

  <div>
  <h3>‚úãRecommended learning resources on affective computing:</h3> 
  <p><a href="https://www.youtube.com/watch?v=5xF0VkB9k88">this video</a> and<a href="https://www.youtube.com/watch?v=I84eJxjrxWc">this video </a> from MIT media lab</p>
</div>
  
  
     
  <div> 
    Homework:
   <br />
    - Start preparing for your <a href="https://moodle.arts.ac.uk/mod/resource/view.php?id=1559297">final project</a>! 
  <br />
    - If you plan to work with content we have covered so far, send me a small proposal on what you plan to make and I'll recommend relevant resources!
  <br />
    - If you plan to work with content we have not covered so far, send me a message and I'll recommend early readings/preparation for you!
  </div>

   <div> 
    üï∂Ô∏è What we have learnt today:
       
      <br /> - Affective computing models human emotion in machines
       <br /> - Modalities: face, speech, text, physiology, multimodal
      <br /> - Used in games, wellbeing, education, companionship, etc.
     <br /> - High potential but high ethical responsibility
  </div>

  <div> 
    We'll see you next Monday same time and same place!
  </div>
  
</body>
</html>
