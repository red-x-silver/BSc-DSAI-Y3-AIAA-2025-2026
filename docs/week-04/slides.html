<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
  <title>Artificial Intelligence and Advanced Analytics 03</title>
  <!-- This whole presentation was made with Big: https://github.com/tmcw/big -->
  
  <link href="../big/big.css" rel="stylesheet" type="text/css" />
  <script src="../big/big.js"></script>
  <link href="../big/themes/lightWhite.css" rel="stylesheet" type="text/css" />

  <!-- Favicon link below, via https://emojitofavicon.com -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%2210 0 100 100%22><text y=%22.90em%22 font-size=%2290%22>üç¥</text></svg>"></link>
  
  <style>
    body {
      background-color: #EDDD6E;
      font-family: -apple-system, BlinkMacSystemFont, avenir next, avenir, helvetica neue, helvetica, Ubuntu, roboto, noto, segoe ui, arial, sans-serif;
      /* font-weight: 700;*/
      margin: 0;
      padding: 0;
      font-style: normal;
      font-size: 21px;
    }
  </style>
  
</head>
<body>
  <div>
    AIAAüëê
    <br />
    Lecture 04
    <br />
    AI applications in the audio domain üé§   
  </div>
    
  <div>Welcome üë©‚Äçüé§üßë‚Äçüé§üë®‚Äçüé§</div>
  
  <div> By the end of this lecture, we'll have learnt about:
  
     <br /> The theoretical: 
     <br /> - Introduction to speech-to-text models
     <br /> - Introduction to text-to-speech synthesis models
     <br />  - Introduction to voice clone applications
    
     <br /> The practical: 
     <br /> - build a Web App that uses OpenAI's Whisper API
  
  </div>
  
  <div> First of all, don't forget to confirm your attendence on <a href="https://www.arts.ac.uk/study-at-ual/course-regulations/attendance-policy/attendance-monitoring"> SEAtS App!  </a></div>

   <div>
 Today we move from computer vision to the audio domain!
     <br /> - Audio ‚â† just sound ‚Äî it can be speech, music, environment sound, ambient noise, bird singing, etc
      <br /> - and for today we'll zoom into the realm of speech!
     
</div>
    
  <div>
  <h2>üé§SPEECH!üé§</h2>
  <p>Speech is one of the most natural human communication forms.</p>
  <p>It carries linguistic content, emotion, and identity etc.</p>
  <p>AI application in speech aims to make computers understand and produce human speech naturally.</p>
</div>




<div>
  <h3>What applications can AI models do with speech? </h3>
  <p>Key applications include:</p>
  <ul>
    <li>Speech-to-text transcription, also called Automatic Speech Recognition</li>
    <li>Text-to-speech synthesis</li>
    <li>Voice Cloning </li>
    <li>Speech Translation </li>
    <li>Emotion Analysis (to be covered in week 08) </li>
  </ul>
</div>


<div>
  <h3>Speech-to-text models</h3>
  <p>- Transforms spoken language into written text.</p>
  <p>- Used in transcription, voice assistants, meeting notes, accessibility tools.</p>
  <p>- also called Speech Recognition models </p>
</div>


<div>
  <h3>How STT Works - the traditional way </h3>
  <p>1. Audio signal processing ‚Üí extract features (MFCCs, spectrograms, more on these later)</p>
  <p>2. Acoustic model ‚Üí map sounds to phonemes </p>
  <p>3. Language model ‚Üí predict words and grammar</p>
  <p>4. Decoder ‚Üí generate text output</p>
  <p> - Traditional models used HMM + GMM.</p>
</div>


<div>
  <h3>How STT Works - the state-of-the-art way</h3>  
  <p>- Modern systems use deep neural networks (CNNs, RNNs, Transformers).</p>
  <p>- End-to-end training replaces hand-crafted pipelines.</p>
  <p>- End-to-end: using audio/audio features as input to directly predict into the text output, instead of going through the intermediate steps of phonemes, words, grammar. </p>
</div>

  <div>
  <h3>SOTA, industry-level STT models include: </h3>  
  <p>- <a href="https://openai.com/index/whisper/">Whisper by OpenAI</a> </p>
  <p>- <a href="https://huggingface.co/nvidia/canary-1b">Canary 1B by NVIDIA </a></p>
  <p>- <a href="https://github.com/modelscope/FunASR">FunASR</a></p>
</div>
  
<div>
  <h3>Whisper by OpenAI</h3>
  <p>- Whisper is an open multilingual speech recognition model.</p>
  <p>- Trained on 680k hours of diverse data.</p>
  <p>- It enjoys a sequence-to-sequence Transformer architecture.</p>
  <p>- Quite robust to accents, noise, and technical vocabulary.</p>
</div>


<div>
  <h3>Practical: Using Whisper API</h3>
  <p>Whisper API can transcribe audio to text in seconds.</p>
  <p>We'll build a simple web app that:</p>
  <ul>
    <li>Uploads an audio file</li>
    <li>Calls Whisper API</li>
    <li>Receives the transcription response </li>
    <li>Displays transcription</li>
  </ul>
</div>


<div>
  <h3>From Speech-to-Text to Understanding</h3>
  <p>Once we have transcribed text, NLP models can analyze meaning.</p>
  <p>Combining ASR (Automatic Speech Recognition) + LLMs ‚Üí speech understanding.</p>
</div>

<div>Move from Speech-to-Text, to Text-to-Speech</div>


<div>
  <h3>Text-to-Speech (TTS)</h3>
  <p>Converts text input into natural-sounding speech, and speech is in the audio domain.</p>
  <p>Used in audio agent (Siri, Alexa), audiobooks, accessibility tools, etc.</p>
</div>


<div>
  <h3>How TTS Works</h3>
  <p>1. Text analysis ‚Üí normalize text, expand numbers, detect punctuation.</p>
  <p>2. Linguistic features ‚Üí phonemes, prosody patterns.</p>
  <p>3. Acoustic model ‚Üí predict mel-spectrogram.</p>
  <p>4. Vocoder ‚Üí generate audio waveform from spectrogram.</p>
</div>


<div>
  <h3>Neural TTS Models</h3>
  <p>Traditional: Concatenative or parametric synthesis.</p>
  <p>Modern: Neural TTS using deep learning (Tacotron, FastSpeech, VITS).</p>
  <p>Produces human-like intonation and voice quality.</p>
</div>


<div>
  <h3>Tacotron 2 Example</h3>
  <p>Google's Tacotron 2 uses sequence-to-sequence models:</p>
  <ul>
    <li>Encoder: processes text (characters/phonemes)</li>
    <li>Decoder: predicts mel-spectrogram frames</li>
    <li>Vocoder: (e.g., WaveGlow) generates audio</li>
  </ul>
</div>


<div>
  <h3>Recent Advancements: VITS</h3>
  <p>VITS combines TTS and vocoder into a single end-to-end model.</p>
  <p>It‚Äôs fast, expressive, and can generate high-quality voices.</p>
</div>


<div>
  <h3>Voice Cloning</h3>
  <p>Voice cloning aims to replicate a specific person‚Äôs voice.</p>
  <p>Uses a few seconds of target voice to adapt a TTS model.</p>
  <p>Applications: personalization, dubbing, accessibility ‚Äî and ethical concerns.</p>
</div>

<div>
  <h3>Mechanism of Voice Cloning</h3>
  <p>1. Speaker embedding extraction from sample audio.</p>
  <p>2. Conditioning the TTS model on this embedding.</p>
  <p>3. Model generates speech in the target voice.</p>
</div>


<div>
  <h3>Zero-Shot Voice Cloning</h3>
  <p>Modern systems can mimic voices without retraining.</p>
  <p>Example: OpenAI's Voice Engine and Microsoft‚Äôs VALL-E.</p>
  <p>They use embeddings learned from massive speaker datasets.</p>
</div>


<div>
  <h3>Ethical and Security Concerns</h3>
  <p>Voice deepfakes raise issues in consent, fraud, and misinformation.</p>
  <p>Solutions: watermarking, detection models, and voice authenticity verification.</p>
</div>


<div>
  <h3>Bringing It All Together</h3>
  <p>Speech-to-Text + Text-to-Speech enables:</p>
  <ul>
    <li>Conversational AI</li>
    <li>Real-time translation</li>
    <li>Accessibility tools</li>
  </ul>
</div>


<div>
  <h3>Practical Component</h3>
  <p>We‚Äôll build a simple web app using OpenAI Whisper API:</p>
  <ul>
    <li>Frontend for audio upload</li>
    <li>Backend calling the API</li>
    <li>Display transcription results</li>
  </ul>
</div>


<div>
  <h3>Beyond Whisper: Multimodal Speech AI</h3>
  <p>Future direction: combining speech, vision, and text for richer context.</p>
  <p>Example: multimodal assistants that see, hear, and talk.</p>
</div>


<div>
  <h3>Summary</h3>
  <ul>
    <li>Speech is a key AI modality.</li>
    <li>STT enables machines to listen.</li>
    <li>TTS enables machines to speak.</li>
    <li>Voice cloning personalizes interaction.</li>
  </ul>
</div>


<div>
  <h3>Next Steps</h3>
  <p>Explore Whisper API in code.</p>
  <p>Experiment with TTS models (e.g., ElevenLabs, VITS).</p>
  <p>Discuss ethical implications of synthetic voices.</p>
</div>  

  <div> 
    Homework:
   <br />
    - Inference <a href="https://github.com/NVlabs/stylegan3">StyleGAN3</a> or <a href="https://github.com/NVlabs/stylegan2">StyleGAN2</a> (choose one or more pre-trained models from the repos) on your laptop or workstation
   <br />
    - Train a <a href="https://github.com/red-x-silver/PokeGAN">Pokemon GAN</a> on your laptop or the workstation. Show me your pokemons!
   <br />
     - Inference a stable diffusion model from your laptop with the <a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/overview">hugging face library for stable diffusions</a>, it is SO HANDY!
    <br />
    - SEND ME your dev note by next Monday!
    <br />
    - [Optional but highly rewarding] Train a StyleGANX on the workstation, using a custom dataset (think of an applicatin that is interesting to you!).
  </div>

   <div> 
    üï∂Ô∏è What we have learnt today:
      <br />
   Three types of model family for generating images:
    - VAE
    <br />
    - GAN
    <br />
    - Diffusion
    <br />
    - Inspect and inference examples of industry-level generative model: StyleGAN, Stable Diffusion
  </div>
  
  <div> 
    We'll see you next Monday same time and same place!
  </div>
  
</body>
</html>
